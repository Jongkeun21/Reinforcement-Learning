{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import random\n",
    "import gym\n",
    "import os\n",
    "\n",
    "from gridworld import gameEnv\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method gameEnv.step of <gridworld.gameEnv object at 0x7f3d886ec3c8>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***QNetwork***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork() :\n",
    "    def __init__(self, h_size) :\n",
    "        self.scalarInput = tf.placeholder(shape=[None, 84*84*3], dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1,84,84,3])\n",
    "        \n",
    "        self.conv1 = tf.contrib.layers.convolution2d(inputs=self.imageIn, num_outputs=32, kernel_size=[8,8], stride=[4,4], padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(inputs=self.conv1, num_outputs=64, kernel_size=[4,4], stride=[2,2], padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(inputs=self.conv2, num_outputs=64, kernel_size=[3,3], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(inputs=self.conv3, num_outputs=h_size, kernel_size=[7,7], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        \n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2, env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        \n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        self.QOut = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "        self.predict = tf.argmax(self.QOut, 1)\n",
    "        \n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.QOut, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ-self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Experience replay***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience_buffer() :\n",
    "    def __init__(self, buffer_size=1000) :\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience) :\n",
    "        if len(self.buffer)+len(experience) >= self.buffer_size :\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "            \n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self, size) :\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size, 5])\n",
    "    \n",
    "def processState(states) :\n",
    "    return np.reshape(states, [84*84*3])\n",
    "\n",
    "def updateTargetGraph(tfVars, tau) :\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    \n",
    "    for idx, var in enumerate(tfVars[0:total_vars//2]) :\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau)+((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "        \n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder, sess) :\n",
    "    for op in op_holder :\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Training***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "update_freq = 4\n",
    "y = 0.99    # discounting factor\n",
    "startE = 1\n",
    "endE = 0.1\n",
    "annealing_step = 1000.    # How many steps of training to reduce startE to endE\n",
    "num_episodes = 1000\n",
    "pre_train_steps = 1000\n",
    "max_epLength = 50    # The max allowed length of episode\n",
    "load_model = False\n",
    "path = './dqn'\n",
    "h_size = 512\n",
    "tau = 1e-3    # Rate to update target newtork toward primary network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "======================================\n",
      "======================================\n",
      "======================================\n",
      "======================================\n",
      "======================================\n",
      "======================================\n",
      "======================================\n",
      "======================================\n",
      "======================================\n",
      "Percent:  16.082  %\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(h_size)\n",
    "targetQN = QNetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "trainable = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainable, tau)\n",
    "myBuffer = Experience_buffer()\n",
    "\n",
    "e = startE\n",
    "stepDrop = (startE-endE)/annealing_step    # decaying epsilon to endE\n",
    "\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "if not os.path.exists(path) :\n",
    "    os.mkdir(path)\n",
    "    \n",
    "with tf.Session() as sess :\n",
    "    if load_model :\n",
    "        print('Loading...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps, sess)\n",
    "    \n",
    "    for i in range(num_episodes) :\n",
    "        episodeBuffer = Experience_buffer()\n",
    "        \n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        \n",
    "        while j < max_epLength :\n",
    "            j += 1\n",
    "            \n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps :\n",
    "                a = np.random.randint(0, 4)\n",
    "            else :\n",
    "                a = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput: [s]})[0]\n",
    "                \n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]), [1,5]))\n",
    "            \n",
    "            if total_steps > pre_train_steps :\n",
    "                if e > endE :\n",
    "                    e -= stepDrop\n",
    "                    \n",
    "                if total_steps % (update_freq) == 0 :\n",
    "                    trainBatch = myBuffer.sample(batch_size)\n",
    "                    \n",
    "                    Q1 = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput: np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.QOut, feed_dict={targetQN.scalarInput: np.vstack(trainBatch[:,3])})\n",
    "                    \n",
    "                    end_multiplier = -(trainBatch[:,4]-1)\n",
    "                    doubleQ = Q2[range(batch_size), Q1]\n",
    "                    targetQ = trainBatch[:,2]+(y*doubleQ*end_multiplier)\n",
    "                    \n",
    "                    _ = sess.run(mainQN.updateModel, feed_dict={mainQN.scalarInput: np.vstack(trainBatch[:,0]), mainQN.targetQ: targetQ, mainQN.actions: trainBatch[:,1]})\n",
    "                    \n",
    "                    updateTarget(targetOps, sess)\n",
    "                    \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d :\n",
    "                break\n",
    "                \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        \n",
    "        if i%100 == 0 :\n",
    "            saver.save(sess, path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"======================================\")\n",
    "            \n",
    "    saver.save(sess, path+'/model-'+str(i)+'.ckpt')\n",
    "    \n",
    "print('Percent: ', sum(rList)/num_episodes, ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
