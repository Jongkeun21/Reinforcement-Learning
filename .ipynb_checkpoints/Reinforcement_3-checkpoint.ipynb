{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Bandit\n",
    "\n",
    "#### - constraints\n",
    "\n",
    "    1) 각 arm은 각기 다른 reward를 제공  \n",
    "    2) 제한된 시간 내에 제한된 횟수만큼 arm을 이용      \n",
    "    3) 한 번에 하나의 arm을 당길 수 있음      \n",
    "    4) bandit은 env.으로부터 random하게 주어짐\n",
    "    \n",
    "#### - Objective\n",
    "\n",
    "    : 정해진 시간 내에 bandit마다 총 reward를 maximize하는 policy를 찾는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contextual_bandit() :\n",
    "    def __init__(self) :\n",
    "        self.state = 0\n",
    "        self.bandits = np.array([\n",
    "            [0.2, 0, 0, -5],\n",
    "            [0.1, -5, 1, 0.25],\n",
    "            [-5, 5, 5, 5]\n",
    "        ])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self) :\n",
    "        self.state = np.random.randint(0, len(self.bandits))\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self, action) :\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        \n",
    "        if result < bandit :\n",
    "            return 1\n",
    "        else :\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***The policy-based agents***\n",
    "\n",
    "##### the agent do -->\n",
    "    1) Observe the env. (Get the current state)      \n",
    "    2) Take an action by policy      \n",
    "    3) update the weight to get optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent() :\n",
    "    def __init__(self, lr, s_size, a_size) :\n",
    "        self.state_in = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        state_in_OH = slim.one_hot_encoding(self.state_in, s_size)\n",
    "    \n",
    "        output = slim.fully_connected(state_in_OH, a_size, biases_initializer=None, activation_fn=tf.nn.sigmoid, weights_initializer=tf.ones_initializer())\n",
    "        \n",
    "        self.output = tf.reshape(output, [-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        self.reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
    "        self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.update = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Training***\n",
    "\n",
    "    1) By getting a state from the environment  \n",
    "    2) Take an action  \n",
    "    3) Receive a reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "cBandit = contextual_bandit()\n",
    "myAgent = agent(lr=1e-3, s_size=cBandit.num_bandits, a_size=cBandit.num_actions)\n",
    "weights = tf.trainable_variables()[0]\n",
    "\n",
    "EPOCHS = 10000\n",
    "rAll = np.zeros([cBandit.num_bandits, cBandit.num_actions])\n",
    "e = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the  3  bandits:  [-0.25  0.    0.  ]\n",
      "Mean reward for each of the  3  bandits:  [ 0.   21.25 41.  ]\n",
      "Mean reward for each of the  3  bandits:  [ 2.5  48.75 75.5 ]\n",
      "Mean reward for each of the  3  bandits:  [  5.    72.   116.75]\n",
      "Mean reward for each of the  3  bandits:  [ 10.    99.25 158.  ]\n",
      "Mean reward for each of the  3  bandits:  [ 13.25 122.   198.5 ]\n",
      "Mean reward for each of the  3  bandits:  [ 21.25 147.   237.  ]\n",
      "Mean reward for each of the  3  bandits:  [ 25.75 175.5  271.5 ]\n",
      "Mean reward for each of the  3  bandits:  [ 38.   199.5  309.75]\n",
      "Mean reward for each of the  3  bandits:  [ 44.75 225.25 347.75]\n",
      "Mean reward for each of the  3  bandits:  [ 51.75 252.   386.  ]\n",
      "Mean reward for each of the  3  bandits:  [ 56.5  273.   425.75]\n",
      "Mean reward for each of the  3  bandits:  [ 66.25 298.5  464.  ]\n",
      "Mean reward for each of the  3  bandits:  [ 78.   324.   503.75]\n",
      "Mean reward for each of the  3  bandits:  [ 82.75 352.   545.  ]\n",
      "Mean reward for each of the  3  bandits:  [ 86.5  377.75 583.5 ]\n",
      "Mean reward for each of the  3  bandits:  [ 89.75 401.5  626.  ]\n",
      "Mean reward for each of the  3  bandits:  [ 92.75 425.25 666.75]\n",
      "Mean reward for each of the  3  bandits:  [ 94.5  452.25 703.5 ]\n",
      "Mean reward for each of the  3  bandits:  [ 99.   481.75 740.  ]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess :\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(EPOCHS) :\n",
    "        s = cBandit.getBandit()\n",
    "        \n",
    "        if np.random.rand(1) < e :\n",
    "            action = np.random.randint(cBandit.num_actions)\n",
    "        else :\n",
    "            action = sess.run(myAgent.chosen_action, feed_dict={myAgent.state_in: [s]})\n",
    "            \n",
    "        reward = cBandit.pullArm(action)\n",
    "        \n",
    "        feed_dict = {myAgent.reward_holder: [reward],\n",
    "                    myAgent.action_holder: [action],\n",
    "                    myAgent.state_in: [s]}\n",
    "        \n",
    "        _, W = sess.run([myAgent.update, weights], feed_dict=feed_dict)\n",
    "        \n",
    "        rAll[s, action] += reward\n",
    "        \n",
    "        if i%500 == 0 :\n",
    "            print(\"Mean reward for each of the \", cBandit.num_bandits, \" bandits: \", np.mean(rAll, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong\n",
      "Wrong\n",
      "Wrong\n"
     ]
    }
   ],
   "source": [
    "for a in range(cBandit.num_bandits) :\n",
    "    if np.argmax(W[a]) == np.argmin(cBandit.bandits[a]) :\n",
    "        print(\"Right\")\n",
    "    else :\n",
    "        print(\"Wrong\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
